{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3193659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bc8244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahilpandey/miniconda3/envs/gdg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998764991760254}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "print(classifier(\"I love this!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9434c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/Users/sahilpandey/Projects/Sentiment_Analysis/Dataset/labels.csv\"\n",
    "img_path = \"/Users/sahilpandey/Projects/Sentiment_Analysis/Dataset/images\" \n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b49986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index   image_name                                           text_ocr  \\\n",
      "0      0  image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
      "\n",
      "                                      text_corrected     humour  sarcasm  \\\n",
      "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...  hilarious  general   \n",
      "\n",
      "       offensive      motivational overall_sentiment  Humor_label  \\\n",
      "0  not_offensive  not_motivational     very_positive            3   \n",
      "\n",
      "   Sarcasm_label  Offensive_label  sentiment_label  \n",
      "0              1                0                2  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "H = {\n",
    "    \"hilarious\": 3,\n",
    "    \"very_funny\": 2,\n",
    "    \"funny\": 1,\n",
    "    \"not_funny\": 0\n",
    "}\n",
    "df[\"Humor_label\"] = df[\"humour\"].map(H).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "S = {\n",
    "    \"very_twisted\": 3,\n",
    "    \"twisted_meaning\": 2,\n",
    "    \"general\": 1,\n",
    "    \"not_sarcastic\": 0\n",
    "}\n",
    "df[\"Sarcasm_label\"] = df[\"sarcasm\"].map(S).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "O = {\n",
    "    \"hateful_offensive\": 3,\n",
    "    \"very_offensive\": 2,\n",
    "    \"slight\": 1,\n",
    "    \"not_offensive\": 0\n",
    "    \n",
    "}\n",
    "df[\"Offensive_label\"] = df[\"offensive\"].map(O).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "sentiment_mapping = {\n",
    "    \"very_positive\": 2, \"positive\": 2,\n",
    "    \"very_negative\": 0, \"negative\": 0,\n",
    "    \"neutral\":1\n",
    "}\n",
    "df[\"sentiment_label\"] = df[\"overall_sentiment\"].map(sentiment_mapping).fillna(0).astype(int)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print(df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cef27bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, tokenizer=None):\n",
    "        self.data = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.data.iloc[idx]['image_name'])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            image = torch.zeros((3, 224, 224))  # fallback if image fails to load\n",
    "\n",
    "        text = str(self.data.iloc[idx]['text_corrected'])\n",
    "        text_encoded = self.tokenizer(text, padding='max_length', truncation=True, max_length=256, return_tensors='pt')\n",
    "        text_encoded = {k: v.squeeze(0) for k, v in text_encoded.items()}  # remove batch dim\n",
    "\n",
    "        # Labels as tensors\n",
    "        sentiment = torch.tensor(int(self.data.iloc[idx]['sentiment_label']), dtype=torch.long)\n",
    "        humor     = torch.tensor(int(self.data.iloc[idx]['Humor_label']), dtype=torch.long)\n",
    "        sarcasm   = torch.tensor(int(self.data.iloc[idx]['Sarcasm_label']), dtype=torch.long)\n",
    "        offense   = torch.tensor(int(self.data.iloc[idx]['Offensive_label']), dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text_encoded,\n",
    "            'sentiment_label': sentiment,\n",
    "            'humor_label': humor,\n",
    "            'sarcasm_label': sarcasm,\n",
    "            'offense_label': offense,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3cc7f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahilpandey/miniconda3/lib/python3.12/site-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from transformers import ViTModel, BertModel, ViTFeatureExtractor, BertTokenizer\n",
    "\n",
    "vit_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a054d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_dataset = MemeDataset(train_df, img_path, transform=transform,tokenizer=bert_tokenizer )\n",
    "test_dataset = MemeDataset(test_df, img_path, transform=transform,tokenizer=bert_tokenizer )\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc776d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, BertModel\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim_q, dim_kv, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim_q, kdim=dim_kv, vdim=dim_kv, num_heads=num_heads, batch_first=True)\n",
    "    \n",
    "    def forward(self, query, key_value):\n",
    "        out, _ = self.attn(query, key_value, key_value)\n",
    "        return out\n",
    "\n",
    "class MemeCrossAttentionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.cross_attn_img_to_txt = CrossAttentionBlock(dim_q=768, dim_kv=768)\n",
    "        self.cross_attn_txt_to_img = CrossAttentionBlock(dim_q=768, dim_kv=768)\n",
    "\n",
    "        self.classifier_input_dim = 768 * 2 \n",
    "\n",
    "        self.sentiment_classifier = nn.Linear(self.classifier_input_dim, 3)\n",
    "        self.humor_classifier     = nn.Linear(self.classifier_input_dim, 4)\n",
    "        self.sarcasm_classifier   = nn.Linear(self.classifier_input_dim, 4)\n",
    "        self.offense_classifier   = nn.Linear(self.classifier_input_dim, 4)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        vit_out = self.vit(pixel_values=image).last_hidden_state      \n",
    "        bert_out = self.bert(**text).last_hidden_state                \n",
    "\n",
    "        img_cls = vit_out[:, 0:1, :]      \n",
    "        text_cls = bert_out[:, 0:1, :]    \n",
    "\n",
    "        img_attn = self.cross_attn_img_to_txt(img_cls, bert_out)  \n",
    "        text_attn = self.cross_attn_txt_to_img(text_cls, vit_out)  \n",
    "\n",
    "        combined = torch.cat([img_attn.squeeze(1), text_attn.squeeze(1)], dim=-1)  \n",
    "\n",
    "        return {\n",
    "            'sentiment_logits': self.sentiment_classifier(combined),\n",
    "            'humor_logits': self.humor_classifier(combined),\n",
    "            'sarcasm_logits': self.sarcasm_classifier(combined),\n",
    "            'offense_logits': self.offense_classifier(combined),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf42bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/20:   0%|          | 0/350 [01:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m loss_offense   = ce_loss(outputs[\u001b[33m'\u001b[39m\u001b[33moffense_logits\u001b[39m\u001b[33m'\u001b[39m], batch[\u001b[33m'\u001b[39m\u001b[33moffense_label\u001b[39m\u001b[33m'\u001b[39m].to(device))\n\u001b[32m     37\u001b[39m loss = loss_sentiment + loss_humor + loss_sarcasm + loss_offense\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m optimizer.step()\n\u001b[32m     41\u001b[39m total_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "model = MemeCrossAttentionClassifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch['image'].to(device)\n",
    "        text_input = {k: v.to(device) for k, v in batch['text'].items()} \n",
    "\n",
    "        outputs = model(images, text_input) \n",
    "\n",
    "        loss_sentiment = ce_loss(outputs['sentiment_logits'], batch['sentiment_label'].to(device))\n",
    "        loss_humor     = ce_loss(outputs['humor_logits'], batch['humor_label'].to(device))\n",
    "        loss_sarcasm   = ce_loss(outputs['sarcasm_logits'], batch['sarcasm_label'].to(device))\n",
    "        loss_offense   = ce_loss(outputs['offense_logits'], batch['offense_label'].to(device))\n",
    "\n",
    "        loss = loss_sentiment + loss_humor + loss_sarcasm + loss_offense\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"[Epoch {epoch+1}] Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            text_input = {k: v.to(device) for k, v in batch['text'].items()}\n",
    "\n",
    "            outputs = model(images, text_input)\n",
    "\n",
    "            val_loss_sentiment = ce_loss(outputs['sentiment_logits'], batch['sentiment_label'].to(device))\n",
    "            val_loss_humor     = ce_loss(outputs['humor_logits'], batch['humor_label'].to(device))\n",
    "            val_loss_sarcasm   = ce_loss(outputs['sarcasm_logits'], batch['sarcasm_label'].to(device))\n",
    "            val_loss_offense   = ce_loss(outputs['offense_logits'], batch['offense_label'].to(device))\n",
    "\n",
    "            total_val_loss += (val_loss_sentiment + val_loss_humor + val_loss_sarcasm + val_loss_offense).item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"[Epoch {epoch+1}] Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(avg_val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
